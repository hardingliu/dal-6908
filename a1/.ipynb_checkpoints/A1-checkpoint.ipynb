{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKGsmsab5I1S"
   },
   "source": [
    "# <center>Assignment 1</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q5VD__J65I1U"
   },
   "source": [
    "There are 2 main parts asked in this assignment - Tensorflow Basics and Neural Networks. You can choose to code in Python2 or Python3. All the imports made in this notebook are as below; if these imports work, you are (mostly) set to complete the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "B4teB1gh5I1V"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function,division\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWzfE86tusct"
   },
   "source": [
    "## Tensorflow - Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtgV4CJM5I1Z"
   },
   "source": [
    "### I. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "waXA1j7A5I1a"
   },
   "source": [
    "<b>1a. Creating Sample Data </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LrDL6noo5I1b"
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(100,3) # 100 data points of dimension 3\n",
    "w = np.array([[1],[2],[3]])\n",
    "b = 10\n",
    "y = np.dot(x, w) + b # Write code to create the target. Use Numpy operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J6m9Ibn65QcP"
   },
   "source": [
    "**1b. Plot Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lS4Hlv8k5UeZ"
   },
   "outputs": [],
   "source": [
    "# Explore the data by plotting whatever makes you understand the problem better. \n",
    "# Your code here.\n",
    "\n",
    "# Plot the first axis of x against y.\n",
    "plt.subplot(121)\n",
    "plt.plot(x[:,0], y, 'bs')\n",
    "plt.title(\"x_0 versus y\")\n",
    "plt.xlabel(\"x_0\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "# Plot the second axis of x againts y.\n",
    "plt.subplot(122)\n",
    "plt.plot(x[:,1], y, 'bs') \n",
    "plt.title(\"x_1 versus y\")\n",
    "plt.xlabel(\"x_1\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the third axis of x againts y.\n",
    "plt.subplot(111)\n",
    "plt.plot(x[:,2], y, 'bs')\n",
    "plt.title(\"x_2 versus y\")\n",
    "plt.xlabel(\"x_2\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qo8hagle5I1c"
   },
   "source": [
    "<b>2. Creating Placeholders</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9aM6fyvd5I1e"
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32,shape=[None,3]) \n",
    "Y_Expected = tf.placeholder(dtype=tf.float32,shape=[None,1]) # Write code to create the placeholder for target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IIcGDu2s5I1f"
   },
   "source": [
    "<b>3. Creating Variables</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OJd_anpk5I1h"
   },
   "outputs": [],
   "source": [
    "b = tf.Variable(dtype=tf.float32,initial_value=np.zeros(shape=(1,1)),name=\"b\")\n",
    "W = tf.Variable(dtype=tf.float32,initial_value=np.zeros(shape=(3,1)),name=\"w\") # Write code to instantiate W with zeros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZItc9VZ5I1k"
   },
   "source": [
    "<b> 4. Creating Compute Graph </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HwpRYAc85I1k"
   },
   "outputs": [],
   "source": [
    "Y = tf.matmul(X, W) + b # Define the equation to compute the output variable.\n",
    "cost = tf.reduce_mean(tf.square((Y - Y_Expected))) # Define the cost function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XoqEq-Mj5I1m"
   },
   "source": [
    "<b> 5. Training and optimizer </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TkF3TNpt5I1o"
   },
   "outputs": [],
   "source": [
    "# This part has been done for you already! Just run it after you finish coding the above sections. \n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train_op = optimizer.minimize(cost)\n",
    "for epoch in range(30):\n",
    "    epoch_cost,_ = sess.run([cost,train_op],feed_dict={X:x,Y_Expected:y})\n",
    "    print (epoch,epoch_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nnMq3Dxx5I1q"
   },
   "source": [
    "<b> 5. Print out parameters </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Htz0trvg5I1s"
   },
   "outputs": [],
   "source": [
    "# Replace the None with the correct operation. You should get W close to [[1],[2],[3]] and b close to 10. \n",
    "print(\"W:\", W.eval())\n",
    "print(\"b:\", b.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1xXx4-CO5I1v"
   },
   "source": [
    "### II. Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0ZylKoT7v9J2"
   },
   "outputs": [],
   "source": [
    "def ndmatmul():\n",
    "    \"\"\"\n",
    "      # 3d x 2d Matmul operation. \n",
    "      You may find some of these functions useful: einsum, tile, expand_dims.\n",
    "      :return a: Placeholder for 3d tensor [float64]\n",
    "              b: Placeholder for 2d tensor [float64]\n",
    "              c: Matrix Product\n",
    "      \"\"\"\n",
    "    a = tf.placeholder(dtype=tf.float64, shape=(None, None, None))\n",
    "    b = tf.placeholder(dtype=tf.float64, shape=(None, None))\n",
    "    c = tf.einsum('ijk,kl->ijl', a, b)\n",
    "    return a,b,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "oHHeXiBOxyI2"
   },
   "outputs": [],
   "source": [
    "A,B,C = ndmatmul()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Rldu3MIfx3Yi"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "a = np.random.randn(5,2,3)\n",
    "b = np.random.randn(3,1)\n",
    "c = np.matmul(a,b)\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(c.shape)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BX2VDVzx01vp"
   },
   "outputs": [],
   "source": [
    "# Will give error if function not implemented. Your output should match Numpy's output.\n",
    "sess = tf.InteractiveSession()\n",
    "c_tensor = sess.run(C,feed_dict={A:a,\n",
    "                            B:b})\n",
    "print(c_tensor)\n",
    "if (c_tensor-c<10**-10).all():\n",
    "    print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_B3PwLX2mYc"
   },
   "source": [
    "### III. Experiments with Feed-forward NN on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cX231VZk5I13"
   },
   "source": [
    "In this Qn, you will experiment with Feed-forward Neural nets while training on the MNIST dataset. Read more about it <a href = \"https://en.wikipedia.org/wiki/MNIST_database\">here</a>. A random sample of the images has been shown to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GX5Q-nJv5I14"
   },
   "outputs": [],
   "source": [
    "# Load MNIST Data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train_data = mnist.train.images # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(eval_data.shape)\n",
    "print(eval_labels.shape)\n",
    "# Randomly choose 10 images from first 50 images of Train Data.\n",
    "for index,idx in enumerate(random.sample(range(50),10)): \n",
    "    plt.subplot(10,1,index+1)\n",
    "    plt.imshow(train_data[idx].reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6I1p3uH5I18"
   },
   "source": [
    "Fill in the following snippet as per the instructions. \n",
    "* For initialising placeholders, use None to accommodate variable batch_size. \n",
    "* Do not change the seed; use it for comparing epoch-wise loss with your friends.\n",
    "* You can use the following <a href =\"https://www.tensorflow.org/versions/r1.1/get_started/mnist/beginners\">tutorial</a> for reference. Note that they use softmax in their example, while you are required to code Feedforward neural network. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IqWoBVW9276e"
   },
   "outputs": [],
   "source": [
    "def initializer_1(shape):\n",
    "    # Do not change the seed. \n",
    "    np.random.seed(1)\n",
    "    return np.random.randn(*shape)\n",
    "\n",
    "def initializer_2(shape):\n",
    "    # Do not change the seed.\n",
    "    np.random.seed(1)\n",
    "    return 0.01 * np.random.randn(*shape)\n",
    "\n",
    "class MNIST_ANN:\n",
    "    def __init__(self,hidden_units,activations,initializer):\n",
    "        \"\"\"\n",
    "        Initialise the weights and build the compute graph. Use AdamOptimizer with default parameters.\n",
    "        :param hidden_units - list of number of hidden units. \n",
    "               Eg: [10,20] => Layer 1 has 10 hidden units and Layer 2 has 20.\n",
    "        :param activations - list of activations for each of the hidden layers.\n",
    "               Eg: [tf.nn.sigmoid, tf.nn.tanh]\n",
    "        :param intializer - the reference to the function used for intializing the weights\n",
    "        \"\"\"\n",
    "        # Define the placeholders\n",
    "        self.input = tf.placeholder(dtype=tf.float32, shape=(None, 784))\n",
    "        self.expected_output = tf.placeholder(dtype=tf.int32, shape=(None, 10))\n",
    "        \n",
    "        # Initialise the weights and biases. Use zeros for the biases. \n",
    "        weights = []\n",
    "        biases = []\n",
    "        # Initializing the weights and biases for the input layer -> first hidden layer.\n",
    "        weights.append(tf.Variable(dtype=tf.float32, initial_value=initializer((784, hidden_units[0]))))\n",
    "        biases.append(tf.Variable(dtype=tf.float32, initial_value=np.zeros(hidden_units[0])))\n",
    "        \n",
    "        # Loop here.\n",
    "        for i in range(1, len(hidden_units)):\n",
    "            weights.append(tf.Variable(dtype=tf.float32, \n",
    "                                     initial_value=initializer((hidden_units[i - 1], hidden_units[i]))))\n",
    "            biases.append(tf.Variable(dtype=tf.float32, initial_value=np.zeros(hidden_units[i])))\n",
    "        # Initializing the weights and biases for the last hidden layer -> output layer\n",
    "        weights.append(tf.Variable(dtype=tf.float32, initial_value=initializer((hidden_units[-1], 10))))\n",
    "        biases.append(tf.Variable(dtype=tf.float32, initial_value=np.zeros(10)))\n",
    "        \n",
    "        # Build the graph for computing output.\n",
    "        h = self.input\n",
    "        for i in range(0, len(activations)):\n",
    "            h = activations[i](tf.matmul(h, weights[i]) + biases[i])\n",
    "        # For output layer\n",
    "        self.output = tf.matmul(h, weights[-1]) + biases[-1]\n",
    "        \n",
    "        # Define the loss and accuracy here. (Refer Tutorial)\n",
    "        self.cost = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.expected_output, logits=self.output))\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.output, 1), tf.argmax(self.expected_output, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        \n",
    "        # Instantiate the optimizer\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        self.train_op = optimizer.minimize(self.cost)\n",
    "        self.session = tf.Session()\n",
    "        \n",
    "        # Initialize all variables\n",
    "        self.session.run(tf.initialize_all_variables())\n",
    "    \n",
    "    def train(self,train_data,train_labels,eval_data,eval_labels,batch_size,epochs=100):\n",
    "        \"\"\"\n",
    "        Training code.\n",
    "        \"\"\"\n",
    "        sess = self.session\n",
    "\n",
    "        # Slice the data and labels into batches depending on the batch_size.\n",
    "        batches = []\n",
    "        num_batches = train_data.shape[0] // batch_size\n",
    "        for i in range(num_batches):\n",
    "            batch = [train_data[i * batch_size: i * batch_size + batch_size], \n",
    "                     train_labels[i * batch_size: i * batch_size + batch_size]]\n",
    "            batches.append(batch)\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            cost_epoch = 0\n",
    "            for batch in batches:\n",
    "                # Forward Propagate, compute cost and backpropagate.\n",
    "                cost,_ = sess.run([self.cost,self.train_op],feed_dict={self.input:batch[0],\n",
    "                                                             self.expected_output: batch[1]})\n",
    "                cost_epoch += cost\n",
    "            if epoch%10 == 0:\n",
    "                print(\"Train accuracy: {0:.12f}\".format(self.compute_accuracy(train_data,train_labels)))        \n",
    "                print(\"Test accuracy: {0:.12f}\".format(self.compute_accuracy(eval_data,eval_labels)))\n",
    "            print(\"Epoch {0:d}: {1:.8f}\".format(epoch,cost_epoch))\n",
    "        print(\"Train accuracy: {0:.12f}\".format(self.compute_accuracy(train_data,train_labels)))\n",
    "        print(\"Test accuracy: {0:.12f}\".format(self.compute_accuracy(eval_data,eval_labels)))\n",
    "\n",
    "    def compute_accuracy(self,data,labels):\n",
    "        \"\"\"\n",
    "        Fill in code to compute accuracy\n",
    "        \"\"\"\n",
    "        sess = self.session\n",
    "        return sess.run(self.accuracy, feed_dict={self.input: data, self.expected_output: labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_TTASFUA5I1_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ann = MNIST_ANN([10],[tf.nn.sigmoid],initializer_1)\n",
    "ann.train(train_data,train_labels,eval_data,eval_labels,batch_size=10,epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "maX6s20Q5I2B"
   },
   "source": [
    "The expected output for the above snippet is\n",
    "<pre>\n",
    "Train accuracy: 0.780763626099\n",
    "Test accuracy: 0.791599988937\n",
    "Epoch 0: 6768.86486949\n",
    "Epoch 1: 3275.00310887\n",
    "Epoch 2: 2590.16959983\n",
    "Train accuracy: 0.873399972916\n",
    "Test accuracy: 0.876900017262\n",
    "</pre>\n",
    "If you get any other output and you feel you are correct, you can proceed (However, I cannot think of any case where you can get a different output). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iJv-fmOh5I2B"
   },
   "source": [
    "### Answer the following questions by running code snippets. Unless asked explicitly (like in Q1 and Q4), you need to just show the system performance and need not comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mmfIKKtm5I2D"
   },
   "source": [
    "**1. Use 1 hidden layer of 10 hidden units with sigmoid activation and batch_size=10 for this question. Observe the network performance for initializer_1 and initializer_2 and explain the behavior. Why does this happen? What is your guess for tanh and relu? Why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IQ-60H-X5I2D"
   },
   "outputs": [],
   "source": [
    "# Your code here. \n",
    "\n",
    "# ANN with initializer_1\n",
    "ann = MNIST_ANN([10], [tf.nn.sigmoid], initializer_1)\n",
    "print(\"START TRAINING WITH INITIALIZER_1\")\n",
    "ann.train(train_data,train_labels,eval_data,eval_labels,batch_size=10)\n",
    "print()\n",
    "\n",
    "# ANN with initializer_2\n",
    "ann = MNIST_ANN([10], [tf.nn.sigmoid], initializer_2)\n",
    "print(\"START TRAINING WITH INITIALIZER_2\")\n",
    "ann.train(train_data,train_labels,eval_data,eval_labels,batch_size=10)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost of the network is decreasing faster with initializer_2; or the train and test accuracies of the network is increasing faster with initializer_2.\n",
    "\n",
    "The weights initialized by initializer_2 are less than those initialized by initializer_1 (100 times less). Therefore, the values of the neurons in the hidden layer with initializer_2 will be samller than those with initializer_1. After we applying the Sigmoid activation function to the neurons of the hidden layer, the resulting values with initializer_2 will be less than those with initializer_1, according to the curve of Sigmoid function. Thus, the cost with initializer_2 will be less than that with initializer_1. \n",
    "\n",
    "For Tanh and ReLU, my guess is that the cost would decrease much faster than that for Sigmoid. \n",
    "\n",
    "If we take a look at the graph of Tanh, we can see that with same input, the output of Tanh is much samller than that of Sigmoid, which means less cost. \n",
    "\n",
    "For ReLU, I think it will be the most fastest. Since it will only output positive value or zero, which pretty fits the task with classifiction, and also giving less cost compared to other activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eysUpIUF5I2H"
   },
   "source": [
    "<b>2. Play around with different configurations of the system. Spend some time on <a href=\"https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.52239&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\"> Tensorflow Playground </a> to get a feel. Just demonstrate the performance of the system and make observations. No need to make any comments. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CYr-ppoI5I2H"
   },
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "\n",
    "# ANN with three hidden layers (30, 20, 10), initializer_2, and relu as activations\n",
    "ann_0 = MNIST_ANN([30, 20, 10], [tf.nn.relu, tf.nn.relu, tf.nn.relu], initializer_2)\n",
    "print(\"START TRAINING ANN_0 WITH RELU AND INITIALIZER_2\")\n",
    "ann.train(train_data,train_labels,eval_data,eval_labels,batch_size=10)\n",
    "\n",
    "# ANN with two hidden layers (10, 20), initializer_2, and sigmoid and relu as activations\n",
    "ann_1 = MNIST_ANN([10, 20], [tf.nn.sigmoid, tf.nn.relu], initializer_2)\n",
    "print(\"START TRAINING ANN_1 WITH SIGMOID AND RELU, AND INITIALIZER_2\")\n",
    "ann.train(train_data,train_labels,eval_data,eval_labels,batch_size=10)\n",
    "\n",
    "# # ANN with two hidden layers (20, 10), initializer_2, and sigmoid and tanh as activations\n",
    "ann_2 = MNIST_ANN([20, 10], [tf.nn.sigmoid, tf.nn.tanh], initializer_2)\n",
    "print(\"START TRAINING ANN_2 WITH SIGMOID AND TANH, AND INITIALIZER_2\")\n",
    "ann.train(train_data,train_labels,eval_data,eval_labels,batch_size=10)\n",
    "\n",
    "# ANN with two hidden layers (10, 20), initializer_2, and sigmoid and tanh as activations\n",
    "ann_3 = MNIST_ANN([20, 10], [tf.nn.sigmoid, tf.nn.tanh], initializer_2)\n",
    "print(\"START TRAINING ANN_3 WITH SIGMOID AND TANH, AND INITIALIZER_2\")\n",
    "ann.train(train_data,train_labels,eval_data,eval_labels,batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GE-NMOxC5I2J"
   },
   "source": [
    "<b>4. List the problems you faced while experimenting [Loss did not decrease, ran into NaNs, etc]. What conclusions did you make? </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network I used in TensorFlow Playground: 0.01 learning rate, no regularization, 2 hidden layers, and the task is classification.\n",
    "\n",
    "When I used the **Linear** function as the activation functions: after about 300 epochs, the train and test loss did not decrease anymore.\n",
    "\n",
    "When I used the **Sigmoid** function as the activation functions: the train and test loss decreased pretty slow. It took about 2,000 epochs for the network to achieve a 0.029 train and test loss respectively.\n",
    "\n",
    "When I used the **Tanh** function as the activation functions: the train and test loss decreased faster than the network with Sigmoid function. It only took about 200 epochs for the network to achieve a 0.027 train loss and 0.030 test loss respectively.\n",
    "\n",
    "When I used the **ReLU** function as the activation functions: the train and test loss decreased really fast. It only took about 70 epochs for the network to achieve a 0.028 train and test loss respectively.\n",
    "\n",
    "My conclusions: it seems that ReLU has the best performance when the problem is classification; Tanh comes as the second choice, though the loss is decreasing slower with Tanh compared to ReLU; Sigmoid could be used, but its performance is not that good. It takes so long for the train and test loss to decrease to some relatively some number, such as 0.028; Linear should not be used, given the problem itself is not linear. The train and test loss do not decrease anymore after some epochs.\n",
    "\n",
    "Another thing that I found out is that adding more layers does not necessarily imporove the performance of the network. However, it might decrease the performance of the network."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "A1.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
