{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IQaTiatZGmoI"
   },
   "source": [
    "# <center>Assignment 3</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2b-ZK9tuGmoK"
   },
   "source": [
    "This assignment is based on embeddings and CNNs. You can choose to code in Python2 or Python3. All the imports made in this notebook are as below; if these imports work, you are (mostly) set to complete the assignment. You will learn the following:\n",
    "* Making use of embeddings in Tensorflow\n",
    "* Coding CNNs in TF\n",
    "* Intuitions behind working of CNN\n",
    "* Intuitions behind embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Zjqi3WjaGmoL"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function,division\n",
    "import random \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fWPIGJrQCziL"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info >= (3, 0):\n",
    "  from builtins import map as m\n",
    "  def map(f,l):\n",
    "    return list(m(f,l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "In8BxTz4GmoO"
   },
   "source": [
    "## Quick Review questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5WuBRadWGmoO"
   },
   "source": [
    "* If the input volume has dimensions 10 x 10 x 32 (Height x Width x Channels), how many weights will be there in a filter that considers an area of 5 x 5?\n",
    "\n",
    "<b>Answer: 5x5x32.</b>\n",
    "* If input volume has dimensions 10 x 10 x 32 and after convolution we get an output volume of 8 x 8 x 64, how many filters were used? \n",
    "\n",
    "<b>Answer: 64.</b>\n",
    "* What is inverted-dropout? Why is it done? \n",
    "\n",
    "<b>Answer: Inverted-dropout is a way of applying dropout to neural networks by performing dropout and scaling both at training time. In contrast, normal dropout performs dropout at training time, and performs scaling at testing time. Inverted-dropout lets us to do scaling at training time, so we could leave the testing process untouched.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWzfE86tusct"
   },
   "source": [
    "## Sentiment Classification - dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "atbvZSXlGmoQ"
   },
   "source": [
    "We will use movie review dataset taken from http://www.cs.cornell.edu/people/pabo/movie-review-data/. The exact dataset we will use is the Sentence-polarity dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6l_qzOlhGmoR"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for file_,label in zip([\"class_neg.txt\",\"class_pos.txt\"],[0,1]):\n",
    "    lines = open(file_).readlines()\n",
    "    lines = list(map(lambda x:x.strip().replace(\"-\",\" \").split(),lines))\n",
    "    for line in lines:\n",
    "        data.append([line,label])\n",
    "    print(\"Number of reviews of {} = {}\".format(file_[:-4],len(lines)))\n",
    "    print(\"\\tMax number of tokens in a sentence = {}\".format(max(map(lambda x:len(x),lines))))\n",
    "    print(\"\\tMin number of tokens in a sentence = {}\".format(min(map(lambda x:len(x),lines))))\n",
    "random.Random(5).shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKqVN0XTGmoV"
   },
   "source": [
    "Observe that the lengths of sentences are different. In case, we need to vectorize the operations, we need all sentences to be of equal length. Therefore, we will pad all sentences to be of equal length and substitute the padded parts of sentence with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UJSIi3EOGmoV"
   },
   "outputs": [],
   "source": [
    "# See some randomly sampled sentences\n",
    "print(\" \".join(data[random.randint(0,len(data))][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x70n4lA7GmoY"
   },
   "source": [
    "We will work with the sentence as given and not remove any stop-words or punctuation marks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "959vht4CGmoZ"
   },
   "outputs": [],
   "source": [
    "sents = map(lambda x:x[0],data) # all sentences\n",
    "all_words = set()\n",
    "for sent in sents:\n",
    "    all_words |= set(sent)\n",
    "all_words = sorted(list(all_words))\n",
    "vocab = {all_words[i]:i for i in range(len(all_words))}\n",
    "print(\"Number of words : \",len(vocab))\n",
    "train = data[:int(0.8*len(data))]\n",
    "test = data[int(0.8*len(data)):]\n",
    "train_data = []\n",
    "train_targets = []\n",
    "test_data = []\n",
    "test_targets = []\n",
    "for list_all,list_data,list_target,label_list in zip([train,test],[train_data,test_data],[train_targets,test_targets],[\"train\",\"test\"]):\n",
    "    for datum,label in list_all:\n",
    "        list_data.append([vocab[w] for w in datum])\n",
    "        list_target.append([label])\n",
    "    print(label_list)\n",
    "    print(\"\\tNumber of positive examples : \",list_target.count([1]))\n",
    "    print(\"\\tNumber of negative examples : \",list_target.count([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5NZIsmiGmoc"
   },
   "source": [
    "For implementation purposes, we will need an index for the padded word and we will use the index 19757.\n",
    "Note: For a dataset of this <i>small</i> size, we will need to do K-Fold Cross-validation to evaluate the performance. However, we will work with this train-test split for the rest of this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cIWGatDXGmoc"
   },
   "source": [
    "## Simple Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQEg38m6Gmod"
   },
   "source": [
    "<img src=\"https://web.cs.dal.ca/~sastry/cnn_simple.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCvl-t5EGmoe"
   },
   "source": [
    "The above image shows the architecture of the simple model that we will implement for text classification. We are interested in the following hyperparameters apart from the number of filters (which we will set to 1 for this problem):\n",
    "* The span of the filter/the number of words considered for making the prediction.\n",
    "* The size of the stride.\n",
    "* The number of activations selected for feeding into softmax classifier.\n",
    "\n",
    "Before continuing:\n",
    "\n",
    "* Can you reason how the machine is classifying (in the above example)? The values of the activations are color-coded. Is this the only possible way the machine can work? \n",
    "\n",
    "    (Your answer might look like : ... filter is ... template matching ... )\n",
    "\n",
    "<b>Answer: The input sentence has 7 words. The dimension of the training data is set to 5, so for each word, we need to embed it as a row with 5 columns. Thus, our input data is a 7x5 matrix. The filter is of size 2, so it is a 2x5 matrix. We slide the filter from top to bottom on the input data, with a stride of 2, which will result a feature map as a 4x1 matrix. Then, we can do a pooling, which will select the 2 cells with max values, and feed them into the softmax function. After the softmax function, we get two values which will be added to 1, and we select the one with higer value, comparing it with the label of the input data, get the error, and do backpropagation.\n",
    "\n",
    "This is not the only possible way the machine can work.</b>\n",
    "    \n",
    "* Why might order of activations need to be retained?\n",
    "\n",
    "<b>Answer: It won't make a difference</b>\n",
    "\n",
    "* In the code, we will add an additional row of zeros to represent the padded words. Will the zeros of the padded words be updated during back-prop? Why or why not?\n",
    "\n",
    "<b>Answer: No, even though the graident will come, we will use TensorFlow's mechanisms to append zero to it.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrDDcU5GGmoe"
   },
   "source": [
    "First, we will write code which can select k top elements in the order they appeared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8SViy9l_Gmog"
   },
   "outputs": [],
   "source": [
    "def k_max_pool(A,k):\n",
    "    \"\"\"\n",
    "    A = 2 dimensional array (assume that the length of last dimension of A will be always more than k)\n",
    "    k = number of elements.\n",
    "    Return: For every row of A, top k elements in the order they appear.\n",
    "    \"\"\"\n",
    "    assert len(A.get_shape())==2\n",
    "    def func(row):\n",
    "        \"\"\"\n",
    "        Hint : I used top_k and reverse.\n",
    "        I am not sure whether the order of the indices are retained when sorted = False in top_k. (did not find any documentation)\n",
    "        Therefore, I suggest that you sort the indices before selecting the elements from the array(Trick: use top_k again!)\"\"\"\n",
    "        ret_tensor = None\n",
    "        ## your code here to compute ret_tensor ##\n",
    "        values, indices = tf.nn.top_k(row, k=k)\n",
    "        values_, indices_ = tf.nn.top_k(indices, k=k)\n",
    "        indices_ = tf.reverse(indices_, [0])\n",
    "        ret_list = []\n",
    "        for i in range(k):\n",
    "            ret_list.append(values[indices_[i]])\n",
    "        ret_tensor = tf.convert_to_tensor(ret_list)\n",
    "        return ret_tensor\n",
    "    return tf.map_fn(func,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MayRnDIAGmoi"
   },
   "outputs": [],
   "source": [
    "A = tf.placeholder(shape=[None,None],dtype=tf.float64)\n",
    "top = k_max_pool(A,5)\n",
    "sess = tf.Session()\n",
    "for i in range(1,6):\n",
    "    np.random.seed(5)\n",
    "    l = np.random.randn(i*10,i*10)\n",
    "    top_elements = sess.run(top,feed_dict={A:l})\n",
    "    l = l.tolist()\n",
    "    top_elements2 = np.array(map(lambda x: [x[i] for i in range(len(x)) if x[i]>sorted(x,reverse=True)[5]],l))\n",
    "    # Note that this test assumes that the 6th largest element and 5th largest element are different.\n",
    "    print(((top_elements - top_elements2)<10**-10).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KN66FnNcGmok"
   },
   "outputs": [],
   "source": [
    "def initializer(shape):\n",
    "    xavier = tf.contrib.layers.xavier_initializer(seed=1)\n",
    "    return xavier(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HeXmr83YGmom"
   },
   "outputs": [],
   "source": [
    "class CNN_simple:\n",
    "    def __init__(self,num_words,embedding_size = 30,span=2,k=5):\n",
    "        self.num_words = num_words\n",
    "\n",
    "        # The batch of text documents. Let's assume that it is always padded to length 100. \n",
    "        # We could use [None,None], but we'll use [None,100] for simplicity. \n",
    "        self.input = tf.placeholder(shape=[None,100],dtype=tf.int32)\n",
    "        self.expected_output = tf.placeholder(shape=[None,1],dtype=tf.float32)\n",
    "        \n",
    "\n",
    "        embedding_matrix = tf.Variable(initializer((num_words, embedding_size)), name=\"embeddings\")\n",
    "        # Add an additional row of zeros to denote padded words.\n",
    "        self.embedding_matrix = tf.concat([embedding_matrix, tf.zeros([1, embedding_size], dtype=tf.float32)], 0)\n",
    "\n",
    "        \n",
    "        # Extract the vectors from the embedding matrix. The dimensions should be None x 100 x embedding_size. \n",
    "        # Use embedding lookup\n",
    "        vectors = tf.nn.embedding_lookup(self.embedding_matrix, self.input) # None x 100 x embedding_size\n",
    "        \n",
    "        # In order to use conv2d, we need vectors to be 4 dimensional.\n",
    "        # The convention is NHWC - None (Batch Size) x Height(Height of image) x Width(Width of image) x Channel(Depth - similar to RGB).\n",
    "        # For text, let's consider Height = 1, width = number of words, channel = embedding_size.\n",
    "        # Use expand-dims to modify. \n",
    "        vectors2d = tf.expand_dims(vectors, 1) # None x 1 x 100 x embedding_size\n",
    "        \n",
    "        # Conv2d needs a filter bank.\n",
    "        # The dimensions of the filter bank = Height, Width, in-channels, out-channels(Number-of-Filters).\n",
    "        # We are creating a single filter of size = span. \n",
    "        # So, height = 1, width = span, in-channels = embedding_size ,out-channels = 1. \n",
    "        single_filter = tf.Variable(initializer((1, span, embedding_size, 1)), name=\"filter\")  \n",
    "        bias = tf.Variable(0.0,name=\"bias\") # You need a bias for each filter.\n",
    "        conv_span = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=single_filter,\n",
    "            # Note that the first and last elements SHOULD be 1. \n",
    "            strides=[1, 1, 1, 1], \n",
    "            # This means that we are ok with input size being reduced during the process of convolution.\n",
    "            padding=\"VALID\"\n",
    "        ) # Shape = [None, 1, 99, 1]\n",
    "        \n",
    "        acts = tf.nn.leaky_relu(conv_span+bias)\n",
    "        \n",
    "        # Now, let us extract the top k activations. \n",
    "        # But, we need to first convert acts this into 2-dimensional.  \n",
    "        # Use tf.squeeze. Be sure to specify the squeeze-dimensions. \n",
    "        acts_2d = tf.squeeze(acts, [1, 3])\n",
    "        \n",
    "        # Use k_max_pool to extract top-k activations\n",
    "        input_fully_connected = k_max_pool(acts_2d,k) # None x k\n",
    "        \n",
    "        # Initialize the weight and bias needed for softmax classifier. \n",
    "        softmax_weight = tf.Variable(initializer((k, 2)), name=\"softmax_weight\")\n",
    "        softmax_bias = tf.Variable(np.zeros(shape=(2)), name=\"softmax_bias\", dtype=tf.float32)\n",
    "        \n",
    "        # Write out the equation for computing the logits.\n",
    "        self.output = tf.nn.softmax(tf.matmul(input_fully_connected, softmax_weight) + softmax_bias, axis=1) # Shape = [None, 2]\n",
    "        \n",
    "        # Compute the cross-entropy cost. \n",
    "        # You might either sum or take mean of all the costs across all the examples. \n",
    "        # It is your choice as the test case is on Stochastic Training.\n",
    "        self.cost = -tf.reduce_sum(self.expected_output * tf.log(self.output[:, 1]) \n",
    "                                  + (1 - self.expected_output) * tf.log(self.output[:, 0]))\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.reshape(tf.argmax(self.output, 1),[-1,1]), tf.cast(self.expected_output, dtype=tf.int64))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))        \n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        self.train_op = optimizer.minimize(self.cost)\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def pad(self,data,pad_word,pad_length=100):\n",
    "        for datum in data:\n",
    "            datum.extend([pad_word]*(pad_length-len(datum)))\n",
    "        return data\n",
    "    \n",
    "    def train(self,train_data,test_data,train_targets,test_targets,batch_size=1,epochs=1,verbose=False):\n",
    "        sess = self.session\n",
    "        self.pad(train_data,self.num_words)\n",
    "        self.pad(test_data,self.num_words)\n",
    "        print(\"Starting training...\")\n",
    "        for epoch in range(epochs):\n",
    "            cost_epoch = 0\n",
    "            c = 0\n",
    "            for datum,target in zip([train_data[i:i+batch_size] for i in range(0,len(train_data),batch_size)],\n",
    "                                   [train_targets[i:i+batch_size] for i in range(0,len(train_targets),batch_size)]):\n",
    "                _,cost = sess.run([self.train_op,self.cost],feed_dict={self.input:datum,self.expected_output:target})\n",
    "                cost_epoch += cost\n",
    "                c += 1\n",
    "                if c%100 == 0 and verbose:\n",
    "                    print(\"\\t{} batches finished. Cost : {}\".format(c,cost_epoch/c))\n",
    "            print(\"Epoch {}: {}\".format(epoch,cost_epoch/len(train_data)))\n",
    "            print(\"\\tTrain accuracy: {}\".format(self.compute_accuracy(train_data,train_targets)))\n",
    "            print(\"\\tTest accuracy: {}\".format(self.compute_accuracy(test_data,test_targets)))\n",
    "    \n",
    "    def compute_accuracy(self,data,targets):\n",
    "        return self.session.run(self.accuracy,feed_dict={self.input:data,self.expected_output:targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KN8RoSLoGmoo"
   },
   "outputs": [],
   "source": [
    "c=CNN_simple(len(vocab))\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qAcWbzyGmop"
   },
   "source": [
    "The expected output for the above snippet is\n",
    "<pre>\n",
    "Starting training...\n",
    "\t100 batches finished. Cost : 0.688363179564\n",
    "\t200 batches finished. Cost : 0.695461705327\n",
    "\t300 batches finished. Cost : 0.695902070602\n",
    "\t400 batches finished. Cost : 0.697339072227\n",
    "\t500 batches finished. Cost : 0.698220448136\n",
    "    ...\n",
    "Epoch 0: 0.675099702418\n",
    "\tTrain accuracy: 0.718958854675\n",
    "\tTest accuracy: 0.664322555065   \n",
    "</pre>\n",
    "If you get any other output and you feel you are correct, you can proceed (However, I cannot think of any case where you can get a different output). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rtV-0gInGmoq"
   },
   "source": [
    "## ConvNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xi_un8GvGmor"
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtrPWOepGmor"
   },
   "source": [
    "<img src=\"https://web.cs.dal.ca/~sastry/cnn.png\" style=\"height:40%;width:40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLMzzdMBGmos"
   },
   "source": [
    "Essentially, there are 2 kind of hyper-parameters - the filter size and number of filters of each size. In the image shown, there are 3 filter-sizes - 2,3,4 and number of filters of each size is 2. Once the convolution is obtained, 1-max pooling is done - it basically involves extracting 1 activation from the list of activations which is the maximum activation. The reason we need to do this is to construct the inputs to the softmax layer which are of a fixed size.\n",
    "Read more at https://arxiv.org/pdf/1510.03820.pdf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sqoRWB9wGmot"
   },
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self,num_words,embedding_size = 30):\n",
    "        self.num_words = num_words\n",
    "\n",
    "        # The batch of text documents. Let's assume that it is always padded to length 100. \n",
    "        # We could use [None,None], but we'll use [None,100] for simplicity. \n",
    "        self.input = tf.placeholder(shape=[None,100],dtype=tf.int32)\n",
    "        self.expected_output = tf.placeholder(shape=[None,1],dtype=tf.float32)\n",
    "        \n",
    "\n",
    "        embedding_matrix = tf.Variable(initializer((num_words, embedding_size)), name=\"embeddings\")\n",
    "        # Add an additional row of zeros to denote padded words.\n",
    "        self.embedding_matrix = tf.concat([embedding_matrix, tf.zeros([1, embedding_size], dtype=tf.float32)], axis=0)\n",
    "        \n",
    "        # Extract the vectors from the embedding matrix. The dimensions should be None x 100 x embedding_size. \n",
    "        # Use embedding lookup\n",
    "        vectors = tf.nn.embedding_lookup(self.embedding_matrix, self.input) # None x 100 x embedding_size\n",
    "        \n",
    "        # In order to use conv2d, we need vectors to be 4 dimensional.\n",
    "        # The convention is NHWC - None (Batch Size) x Height(Height of image) x Width(Width of image) x Channel(Depth - similar to RGB).\n",
    "        # For text, let's consider Height = 1, width = number of words, channel = embedding_size.\n",
    "        # Use expand-dims to modify. \n",
    "        vectors2d = tf.expand_dims(vectors, 1) # None x 1 x 100 x embedding_size\n",
    "        \n",
    "        # Create 50 filters with span of 3 words. You need 1 bias for each filter.\n",
    "        filter_tri = tf.Variable(initializer((1, 3, embedding_size, 50)), name=\"weight3\")  \n",
    "        bias_tri = tf.Variable(tf.zeros((1,50)), name=\"bias3\")  \n",
    "        conv1 = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=filter_tri,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\"\n",
    "        )  # Shape = [None, 1, words-2, 50]\n",
    "        A1 = tf.nn.leaky_relu(conv1+bias_tri)\n",
    "\n",
    "        # Create 50 filters with span of 4 words. You need 1 bias for each filter.\n",
    "        filter_4 = tf.Variable(initializer((1,4,embedding_size,50)), name=\"weight4\")  \n",
    "        bias_4 = tf.Variable(tf.zeros((1,50)), name=\"bias4\")\n",
    "        conv2 = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=filter_4,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\"\n",
    "        )  # Shape = [None, 1, words-3, 50]\n",
    "\n",
    "        A2 = tf.nn.leaky_relu(conv2+bias_4)\n",
    "\n",
    "        # Create 50 filters with span of 5 words. You need 1 bias for each filter.\n",
    "        filter_5 = tf.Variable(initializer((1,5,embedding_size,50)), name=\"weight5\")  \n",
    "        bias_5 = tf.Variable(tf.zeros((1,50)), name=\"bias5\")\n",
    "        conv3 = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=filter_5,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\"\n",
    "        )  # Shape = [None, 1, words-4, 50]\n",
    "\n",
    "        A3 = tf.nn.leaky_relu(conv3+bias_5)\n",
    "        \n",
    "        # Now extract the maximum activations for each of the filters. The shapes are listed alongside. \n",
    "        max_A1 = k_max_pool(tf.reshape(tf.squeeze(A1, [1]), [-1, 98 * 50]), 50)  # None x 50\n",
    "        max_A2 = k_max_pool(tf.reshape(tf.squeeze(A2, [1]), [-1, 97 * 50]), 50)  # None x 50\n",
    "        max_A3 = k_max_pool(tf.reshape(tf.squeeze(A3, [1]), [-1, 96 * 50]), 50)  # None x 50\n",
    "        \n",
    "        max_A1 = tf.reshape(tf.squeeze(tf.reduce_max(A1, 2)), [-1, 50])  # None x 50\n",
    "        max_A2 = tf.reshape(tf.squeeze(tf.reduce_max(A2, 2)), [-1, 50])  # None x 50\n",
    "        max_A3 = tf.reshape(tf.squeeze(tf.reduce_max(A3, 2)), [-1, 50])  # None x 50\n",
    "        \n",
    "        concat = tf.concat([max_A1, max_A2, max_A3], axis=1) # None x 150\n",
    "        \n",
    "        # Initialize the weight and bias needed for softmax classifier. \n",
    "        self.softmax_weight = tf.Variable(initializer((150, 2)), name=\"softmax_weight\")\n",
    "        self.softmax_bias = tf.Variable(0.0 * initializer((1, 2)), name=\"softmax_bias\")\n",
    "        \n",
    "        # Write out the equation for computing the logits.\n",
    "        self.output = tf.nn.softmax(tf.matmul(concat, self.softmax_weight) + self.softmax_bias, axis=1) # Shape = [None, 2]\n",
    "        \n",
    "        # Compute the cross-entropy cost. \n",
    "        # You might either sum or take mean of all the costs across all the examples. \n",
    "        # It is your choice as the test case is on Stochastic Training. \n",
    "        self.cost = tf.reduce_sum(-self.expected_output * tf.log(self.output[:, 1]) \n",
    "                                  - (1 - self.expected_output) * tf.log(self.output[:, 0]))\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.reshape(tf.argmax(self.output, 1),[-1,1]), tf.cast(self.expected_output, dtype=tf.int64))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        self.train_op = optimizer.minimize(self.cost)\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def get_distance(word1,word2):\n",
    "        return tf.reduce_sum(np.dot(word1, word2)) / (tf.reduce_sum(tf.sqrt(tf.square(word1))) * tf.reduce_sum(tf.sqrt(tf.square(word2))))\n",
    "    \n",
    "    def get_most_similar(word):\n",
    "        emb = c.session.run(c.embedding_matrix)\n",
    "        word_id = vocab[word]\n",
    "        word_sim = {w:0 for w in vocab}\n",
    "        for w2 in \n",
    "        \n",
    "    def pad(self,data,pad_word,pad_length=100):\n",
    "        for datum in data:\n",
    "            datum.extend([pad_word]*(pad_length-len(datum)))\n",
    "        return data\n",
    "    \n",
    "    def train(self,train_data,test_data,train_targets,test_targets,batch_size=1,epochs=1,verbose=False):\n",
    "        sess = self.session\n",
    "        self.pad(train_data,self.num_words)\n",
    "        self.pad(test_data,self.num_words)\n",
    "        print(\"Starting training...\")\n",
    "        for epoch in range(epochs):\n",
    "            cost_epoch = 0\n",
    "            c = 0\n",
    "            for datum,target in zip([train_data[i:i+batch_size] for i in range(0,len(train_data),batch_size)],\n",
    "                                   [train_targets[i:i+batch_size] for i in range(0,len(train_targets),batch_size)]):\n",
    "                _,cost = sess.run([self.train_op,self.cost],feed_dict={self.input:datum,self.expected_output:target})\n",
    "                cost_epoch += cost\n",
    "                c += 1\n",
    "                if c%100 == 0 and verbose:\n",
    "                    print(\"\\t{} batches finished. Cost : {}\".format(c,cost_epoch/c))\n",
    "            print(\"Epoch {}: {}\".format(epoch,cost_epoch/len(train_data)))\n",
    "            print(\"\\tTrain accuracy: {}\".format(self.compute_accuracy(train_data,train_targets)))\n",
    "            print(\"\\tTest accuracy: {}\".format(self.compute_accuracy(test_data,test_targets)))\n",
    "    \n",
    "    def compute_accuracy(self,data,targets):\n",
    "        return self.session.run(self.accuracy,feed_dict={self.input:data,self.expected_output:targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dfYIkXOkGmov"
   },
   "outputs": [],
   "source": [
    "c=CNN(len(vocab))\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhrJL02kGmow"
   },
   "source": [
    "The expected output for the above snippet is\n",
    "<pre>\n",
    "Starting training...\n",
    "\t100 batches finished. Cost : 0.692921404839\n",
    "\t200 batches finished. Cost : 0.694593518078\n",
    "\t300 batches finished. Cost : 0.695016788642\n",
    "\t400 batches finished. Cost : 0.695038306713\n",
    "\t500 batches finished. Cost : 0.693231915712\n",
    "    ...\n",
    "Epoch 0: 0.571991487547\n",
    "\tTrain accuracy: 0.895532906055\n",
    "\tTest accuracy: 0.759962499142 \n",
    "</pre>\n",
    "If you get any other output and you feel you are correct, you can proceed (However, I cannot think of any case where you can get a different output). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ozMFo7kGmow"
   },
   "source": [
    "### Effect of Batch Size on Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tybsvV02Gmox"
   },
   "source": [
    "Study the effects of changing batch size. Just run the various experiments and observe the results (Run it in non-verbose mode). No need to make any comments here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9JjNaWPtGmoy"
   },
   "outputs": [],
   "source": [
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,batch_size=10)\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,batch_size=20)\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,batch_size=30)\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,batch_size=40)\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,batch_size=50)\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,batch_size=100)\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,batch_size=200)\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,batch_size=300)\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,batch_size=400)\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzOv1TG_Gmoz"
   },
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVha1w_BGmo0"
   },
   "source": [
    "Add 2 functions - get_distance and get_most_similar to the CNN class (the big one). \n",
    "* get_distance(word1,word2) - should return the cosine distance between the 2 words.\n",
    "* get_most_similar(word) - should return top 10 most similar words to the word passed.\n",
    "\n",
    "Now, use the 2 functions to record the distances between a list of word-pairs as the training progresses. (One easy way to go about could be to save the embedding matrix in the hard-disk for every 5 updates.):\n",
    "* Study the distance between words of opposite sentiment as the training progresses. Ex: Good and Bad, Good and horrible, etc.\n",
    "* Study the distance between words of same sentiment. Ex: Good and Beautiful, Bad and Terrible, etc.\n",
    "* Study how the non-sentiment bearing words relate to each other. Ex: his, her, an, it, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zw4bN713Gmo0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMOB3iO5ag_X"
   },
   "source": [
    "### Learnings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bj0s5x-FarR3"
   },
   "source": [
    "List out the observations and conclusions you made from the various experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TpC2Sjpfa4Zj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Zongming Liu - A3.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1Vg_mOVrg8tlz_ihKzNbZynKWVp4SmBZx",
     "timestamp": 1528746692641
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
